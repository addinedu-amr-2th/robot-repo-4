# 코드 소개

# ROS 프로젝트 개발일지
### 사전 준비
- 원래 [Deepspeech stt 모델](https://github.com/sooftware/kospeech)을 이용하여 학습해 사용하려 했으나, 컴퓨팅 파워 부족으로 [이미 학습되어 있는 딥러닝 모델](https://github.com/openai/whisper)을 사용하기로 결정. 성능은 양호해보인다. 
- ChatGPT 사전 리서치 : 비용문제 때문에 연습용으로는 curie 모델을 fine-tuning해서 사용하고, 이후 시연시 davich 모델을 fine-tuning해서 사용하게 될 것으로 생각됨. 이를 위한 fine-tune 데이터셋은 어느정도 제작해둠(시나리오 추가 요구)
- TTS 모델은 눈여겨봐둔 모델이 존재하나 다른 모델을 사용하게 될 것으로 생각됨 

### 2023.5.30
- [audioROS](https://github.com/LCAV/audioROS) 패키지를 활용해서 오디오 퍼블리셔, 서브스크라이버 제작. 일정 시간 퍼블리셔가 구동하다 정지하는 현상 해결
  하지만, 음성이 뚝뚝 끊기는 문제 발생. ROS통신의 속도에 한계가 존재해서 딜레이를 청크단위로 끊어서 그런 것으로 생각됨.
  이 문제를 해결하기 위해선 2가지 솔루션이 존재할 것으로 예상됨
  - 1. 한번에 보내는 사이즈를 키워서 슬로우 모션으로 들리는 상태로 STT 모델에 건네주는 것 (이 경우 느린 음원을 STT 모델이 제대로 알아들을까 미지수)
  - 2. 서브스크라이버 측에서 짧은 시간동안 청크를 모아서 짧은 음원을 만들고 STT 모델에 넣어 1,2 글자를 뱉게 해서 이것을 하나의 문장으로 모으는 것
    (주로 [realtime STT](https://github.com/davabase/whisper_real_time/blob/master/transcribe_demo.py)가 이런 방식을 채택하는 것으로 보임.) 

### 2023.5.31
- 오디오 퍼블리셔 토픽 발행 주기를 짧게 하니, 오디오가 끊기는 문제 해결. 소리가 반복해서 들리나, 이는 스피커에서 나온 소리가 다시 마이크로 들어가 생기는 메아리 현상으로 보인다.
  실제 로봇에 적용하려면 TTS로 말하는 동안에는 음성 입력을 받지 않는 상태로 만들어야 할 것이다(까먹지 말자)
  현재 음성 전송을 잘 하기 위한 조건은 다음과 같다
   - 발행하는 쪽의 chunk size는 작을 수록 매끈하다.
   - 발행하는 주기를 짧게 해야 매끈하게 전송된다.

- 현재 3개의 패키지를 만들었다.
    - audio publisher : 음성을 퍼블리쉬 하는 패키지 (청크 단위로 보내는 stream 모듈이 존재)
      - file : tts 모델이 추가된 하위 모듈이 만들어질 것으로 예상 - 그 때 직접 만들어야 할듯
    - audio subscriber : 음성을 받아서 재생하는 패키지 (현재 음성을 받아들이는 audio subsciber 모듈만 존재, 여기에 stt까지 적용시킨 하위 모듈을 추가할 예정)
    - audio inference : msg등 필요한 도구들이 모여있는 패키지 (위의 두 패키지를 실행하기 위해서 필요)

- STT 모델을 audio subscriber에 넣어 stt_subscriber 모듈을 만들어 봤는데, 주피터로 음성을 녹음해서 해독시켰을 때는 정상 해독했으나
  callback함수로 chunk를 모으다 보내는 방식으로 해봤더니 이상하게 번역하는 문제 발생
  subscriber쪽 문제일텐데 어떻게 수정해야 제대로 알아들을지 감이 안온다..
  - Hello라고 말하는데 I'm Sorry라고 해석한다.
 
### 2023.6.1
- 잡음의 해결법은 soundfile이라는 모듈.
  메세지로 음성을 보내다보니 보내지는 파일은 numpy로 보내지는데 이를 제대로 파일로 저장하는 방법은 이 모듈밖에 없었던 듯 하다.
- STT 모델이 잡음도 멋대로 Transcript해버리는게 골치아프다. 잡음은 좀 모델한테 안보내면 좋겠는데.. numpy 청크 수치를 0로 바꾸면 되려나.. 모르겠다.
- Whisper가 단어단위로 학습된 모듈이라 단어 vocab에 없으면 번역이 제대로 안돼서 고유명사나 가.나.다 같은건 번역이 제대로 안된다.
  언어모델에 전해줄 로봇의 이름을 그냥 로봇이라고 해야할듯..
- 매니퓰레이터, 뎁스 카메라가 존재하는 터틀봇4를 제공받을 수 있을 것 같다. 당분간 터틀봇4를 사용할 수 있게끔 하는게 목표가 될지도

- 일단 1차적인 STT 모듈은 만들어진 것 같다. 이 모듈에 추가해야 하는 기능은
    - 1. 주변 환경음 소리를 없애버리기 (numpy 행렬 개조하면 될지도)
    - 2. 자기 이름이 들릴 때까지는 소리를 듣는 시간을 짧게(시동코드), 이름이 들리고 나면 듣는 시간을 길게 설정하는 부분(실질적 GPT에 넣을 인풋)
    - 3. STT 결과물을 그냥 툭 뱉고 끝이 아니라 스택에 저장해서 길게 만들어 사용하는 부분 (I should buy, an apple from, market => I should buy an apple from market)
    - 4. 그렇게 모인 스택을 GPT API에 넣고 결과물 텍스트를 메세지로 발행하는 부분.. 근데 기본 Subscriber 모듈을 개조해서 만든거라, 여기서 발행까지 되게 하는게 은근 힘들지도.. 

### 2023.6.2
- String msg를 받아 여러 액션을 수행하는 action_maker 패키지를 제작.
  - 인위적으로 sleep을 주지 않으면 바로 다음 스케쥴로 넘어가 이전 스케쥴이 시행되지 못하는 문제가 존재
  - 이 것을 해결하기 위해선 각 스케쥴을 액션으로 처리해서 지속적인 동작과 완료 여부를 알려줘야만 한다.
  - ROS의 액션에 대해서 공부하고 파일을 개조해야..
- STT에서 ChatGPT 모델이 없을 때(비정상상태) 어느정도 기본 동작은 가능하게 초안 개조할 예정

### 2023.6.7
- 액션으로 개조하려 했으나 기초적인 수준의 동작은 완료 시점이 정의되지 않아 그냥 딜레이로 처리해야하는 것을 깨달음. 복잡한 동작의 경우에만 액션으로 처리해야할 것 같다.
- led, 부저 등 low 레벨의 동작을 구현하려 했으나 실패, ROS 아두이노 제어 방법 구조를 파악하기 위해 분석

### 2023.6.8
- action maker : 기본적인 동작 정의, 메세지 받아서 순차 처리하게 정리
- stt_subscriber : gpt 모델이 정상작동하지 않을 경우 대비 코드 동작 확인 완료.
- whisper 모델이 transcript 뿐 아니라 번역작업까지 한다는 사실을 발견, 언어 제한적이지 않은 서비스가 가능할 것 같다.

### 2023.6.9
- 우선순위 높은 명령이 들어올 경우 처리할 수단이 필요. 
  스케쥴을 지금처럼 명령이 들어오면 덮어씌우는 방식이 아니라 만약 우선순위 높은 명령이 들어오면 앞으로 추가, 아니면 뒤로 추가하는 방식으로 스케쥴 메이커 수정할 필요성이 생겼다.
- ChatGPT가 설계할 로직의 '코드블록'을 위해서 시나리오를 생각하자.
- Turtlebot4-lite 제어 테스트 (3개의 회사 제품이 들어있어서 3사의 패키지를 전부 받았다.)
- Turtlebot4 humble에서는 fastrtpDDS를 사용할 것을 권장하나 bring up이 되지 않고 정지해버린다. 아마 아두이노 보드가 처리하지 못해서 다운되는 것으로 보인다.
결과 galactic에서 권장하는 cycloneDDS를 사용하니 작동되는 것을 볼 수 있었다.
- Turtlebot 자체에 서비스 형태로 이미 구성되어있는 기능이나 샘플 코드로 제작되어 있는 기능들이 많아 표현 가능한 동작들이 많아질 것으로 기대된다. 
- 시스템 설계를 위한 UML?로는 '시스템 구성도(H/W(io), S/W)'와 예상기능리스트(가능할 것으로 예상되는 동작)

### 2023.6.13
- turtlebot4 구동 설정 성공. 로봇팔이 가능해질 것 같다.
- 이제 chatgpt API 맥락, 피드백 사용로직 및 fine tuning 준비에 들어갈듯하다
- action maker 명령 우선순위 로직 및 여러 파라미터 받을 수 있게 디폴트 값 추가해서 개조가 필요

### 2023.6.14
- turtlebot4-lite depth 카메라 topic이 나오지 않는 문제 발생. turtlebot4-lite 모델에 달려있는 OKA-D 뎁스 카메라 문제인가싶어 OKA-D github의 코드는 정상 동작하였고, turtlebot4의 설정을 이전버전으로 바꾸니 해결됐다.
- 하지만 뎁스 카메라 이미지를 구독하니 통신이 다운되는 문제 발생. 일반적으로는 이러한 문제로 로봇 1대당 하나의 공유기 같은 것이 내장되어있다고 한다. 
- 이러한 문제로 뎁스 카메라를 쓰기 어려우니 설정에서 빼버린 것으로 추정된다.

### 2023.6.15
- turtlebot4 안에서 뎁스 카메라의 중앙 수치를 토픽으로 쏘고, 요청 받은 좌표의 수치를 서비스로 쏘는 방식을 사용하니 통신망이 다운되지않는다.
- 0이라는 이상값이 자주 나와서 수치가 0일 경우 토픽을 발행하지 않게 하였다.

### 2023.6.16~18
- 맵을 만들고 네비게이션을 시도.
- teleop key로 조작하고 SLAM으로 맵을 그리는데는 성공하였다.
- 하지만 정작 네비게이션을 하려면 odom과 map의 관계가 정의되지 않아 fixed frame을 map으로 하면 rviz에 로봇이 나오지 않고, 반대로 로봇의 base_frame을 fixed frame으로 하면 map이 나오지 않는다.
- 임의로 둘 사이의 관계를 만들어 보낼 경우, 맵상에 로봇이 나오고, 목표로 이동하는 것도 가능하나, 이는 정상적인 위치관계가 아니기에 장애물을 피하지 못하고 벽에 박아버린다.
- cycloneDDS로 멋대로 바꾼게 문제였거나, 기존 minibot 패키지와 충돌이 나서 일어나는 문제로 추정되나 포멧을 하고 테스트를 하기에는 너무 멀리 와버렸다.
- 그 동안 태웅씨는 fine tuning 리서치를 해서 감정 분류 task만 수행하는 로봇팔 파트 초안 완성.
test 결과 fine tuning 후에는 옳바른 사용을 한다는 가정하에 학습 데이터로 주어진 문법을 절대로 벗어나지 않는다는 것을 확인할 수 있었다.
  
### 2023.6.19
- 태웅씨의 설득으로 turtlebot4를 포기하기로 결정. 작은 기체인 minibot으는 로봇팔도 포기해야 하는거 아닌가 걱정했으나,
결합한 후 확인해보니 걱정과 달리 주행 중 로봇팔이 동작해도 쓰러지지 않는다. 
- 라이다가 가려져 있어서 어떻게 될까 맵에 검은 선 나오고 설정 건들여야 하는거 아닌가 걱정하며 팔을 바닥에 붙이고 이동하는 안을 테스트 해보니 회전관성이 높아져서 바퀴가 엄청 느리게 돈다.
- SLAM을 테스트 해보니 앞쪽에 파티클이 안 뿌려질 뿐 맵이 매우 잘 그려진다. 
네비게이션을 해보니 완전 전방 장애물을 인식하지 못할 뿐 목표 지점까지 잘 이동한다.

### 2023.6.20
- 미니봇을 사용하기로 채택된 이상 빠르게 미니봇을 동작시키는 스케쥴 메이커를 제작.
- 저번 스케쥴 메이커와는 다르게 이번 스케쥴 메이커는 회의에서 이야기 나온 우선도를 적용시키기 위해 별도의 멤버를 갖고 우선도에 따라 기존 스케쥴의 앞 뒤로 신규 스케쥴을 추가하는 기능을 제작한다.
- 그 결과 필연적으로 단순히 앞에서부터 재생하던 초안과는 압도적으로 복잡도가 늘어나게 된다.
- 완료 신호를 토픽으로 받음으로 다음 동작을 수행하는 시스템을 순차적 동작 수행 방법으로 채택했으나, 매번 비어있는 스케쥴 목록에 처음 스케쥴이 들어왔을 때 트리거를 주는 것이 까다롭다...
- 일단 우선도 높음은 비상 정지 시나리오에서 발상된 것이기에 성공 여부가 실패로 들어오면 기존의 태스크를 전부 취소하기로 결정했다.

### 2023.6.21
- ChatGPT 리서치.
- ChatGPT(GPT 3.5 /GPT 4)는 ChatCompletion이라는 엔드포인트로, GPT3 (Ada, Babbage, Curie, Davinci)는 Completion이라는 엔드포인트로 동작한다.
- 결과 작동방식, 사용 데이터, 파라미터가 전부 다르기에 GPT 노드는 사용하는 모델에 따라 별도의 클래스를 사용한다.
  - ChatGPT의 경우, `system` 키워드를 통해 역할을 부여하는 방식. 논리적이고 지성이 느껴지나, 알려준 문법에서 탈출하는 문제가 잦다.
  - GPT3의 경우 `Fewshot-learning'을 통해 예시를 들어주면 스타일을 따라하는 방식. 문법은 잘 지키나, 논리성을 보기 힘들다.
- GPT3과 ChatGPT3.5의 경우 기본적으로 차이는 어마어마한 양의 finetuning데이터라고 한다.
모델의 성능은 finetuing을 할 경우 압도적으로 증가하고, fewshot learning을 하면 더욱 압도적으로 증가한다고 한다.
GPT4의 경우 모델부터 근본적으로 달라서 전문성이 다른 여러 LLM의 앙상블 같은 구조라고 한다.
다른 ChatGPT 사용 논문들에서 GPT4 + fewshot learning을 사용하는 것으로 보아 성능은 압도적으로 보이나 GPT4의 API는 신청 후 통과 절차를 밟아야만 사용 가능하다. 우리 프로젝트 기간 내에는 절대 사용 불가
- GPT3.5에 finetuning을 하는 것이 가장 이상적으로 보이나, ChatGPT 3.5와 4는 Openai에서 finetuning 엔드포인트를 열어주지 않는다.
- 웹 사이트에서 ChatGPT를 사용할 경우 대화 맥락을 기억하는 모습을 보이기에 이러한 기능을 기대했으나, 전혀 기억하지 못한다.
ChatGPT 웹사이트는 내부적으로 여태까지 대화내역을 가능한 모아 모델에 보내 사용하는 듯하다.
- 따라서 우리도 이러한 맥락을 기억하기 위해서는 여태까지의 대화 내역을 모아 매번 보내는 방법이 필요하나, 이러한 방식을 사용하면 호출시마다 사용 토큰량 상승에 의한 비용 상승 문제가 생길 수 있다. 
이를 해결하기 위해서는 기억되는 데이터 양을 한정시키고 이를 넘어가면 이전 대화 내역을 지워야 한다. 이를 단기기억이라 하자. 
- 하지만 이러한 방법으로는 오래 기억해야만하는 데이터마저 잊혀져버린다. 따라서 별도의 텍스트를 만들어 중요한 정보를 들으면 GPT 클래스가 스스로 자신의 멤버변수 텍스트를 수정하는 기능을 호출하게 만들자. 이를 장기기억이라 하자.
-   
